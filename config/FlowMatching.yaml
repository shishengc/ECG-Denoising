
#type: args

train:
  epochs: 100000
  batch_size: 64
  gradient_accumulate_every: 1
  valid_epoch_interval: 5000
  condition: true
  use_ema: true

  optimizer:
    type: "AdamW"
    lr: 1.0e-5
    weight_decay: 0.

base_model:
  dim: 64
  out_dim: 1
  channels: 1
  self_condition: true
  condition: true

adapt_scheduler:
  num_layers: 3
  dim: 64
  head_size: 32
  num_heads: 4
  ff_dim: 256

flow:
  sigma: 0.0
  odeint_kwargs:
    method: "euler"
  num_channels: 1
  sampling_timesteps: 4
  default_use_ode: false

test:
  batch_size: 64