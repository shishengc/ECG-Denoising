
#type: args

train:
  steps: 25000
  batch_size: 64
  gradient_accumulate_every: 1
  val_interval: 1
  condition: true
  use_ema: true

  optimizer:
    type: "AdamW"
    lr: 7.5e-5
    weight_decay: 0.

base_model:
  dim: 64
  out_dim: 1
  channels: 1
  self_condition: false
  condition: true


flow:
  sigma: 0.0
  odeint_kwargs:
    method: "euler"
  num_channels: 1
  sampling_timesteps: 4
  default_use_ode: false

test:
  batch_size: 64