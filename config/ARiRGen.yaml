
#type: args

train:
  steps: 50000
  batch_size: 256
  gradient_accumulate_every: 1
  val_interval: 200
  condition: true
  use_ema: true

  optimizer:
    type: "AdamW"
    lr: 9.0e-5

base_model:
  dim: 64
  out_dim: 1
  channels: 1
  z_channels: 1
  condition: true
  self_condition: true

flow:
  sigma_max: 1.0
  sigma_min: 1.0e-5
  odeint_kwargs:
    method: "euler"
  num_channels: 1
  sampling_timesteps: 10
  default_use_ode: false

test:
  batch_size: 64